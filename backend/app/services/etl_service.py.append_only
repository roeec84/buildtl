
    @staticmethod
    def write_sink_data(df, datasource, table_name, mode='append'):
        """
        Write DataFrame to sink datasource.
        """
        from app.core.security import decrypt_value
        import tempfile
        import json
        
        db_type = datasource.source_type
        config = datasource.connection_config.copy()
        
        # Decrypt sensitive fields
        if 'password' in config:
            config['password'] = decrypt_value(config['password'])
        if 'credentials_json' in config:
            config['credentials_json'] = decrypt_value(config['credentials_json'])
            
        if db_type in ['postgresql', 'mysql', 'sql_server', 'azure_sql']:
            connection_string = ETLService._build_connection_string(db_type, config)
            # Parse connection string to get JDBC URL (reusing logic from load_source_data)
            if db_type == 'postgresql':
                jdbc_url = connection_string.replace('postgresql://', 'jdbc:postgresql://')
            elif db_type == 'mysql':
                jdbc_url = connection_string.replace('mysql+pymysql://', 'jdbc:mysql://')
            elif db_type in ['sql_server', 'azure_sql']:
                jdbc_url = connection_string.replace('mssql+pyodbc://', 'jdbc:sqlserver://')
            
            df.write \
                .format("jdbc") \
                .option("url", jdbc_url) \
                .option("dbtable", table_name) \
                .option("user", config.get('username', '')) \
                .option("password", config.get('password', '')) \
                .mode(mode) \
                .save()
                
        elif db_type == 'bigquery':
            project_id = config.get('project_id')
            dataset_id = config.get('dataset_id')
            
            # Credentials handling
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(json.loads(config['credentials_json']), f)
                credentials_path = f.name
            
            full_table_id = f"{dataset_id}.{table_name}"
            if project_id:
                full_table_id = f"{project_id}.{full_table_id}"
            
            writer = df.write \
                .format("bigquery") \
                .option("credentialsFile", credentials_path) \
                .option("temporaryGcsBucket", "dataproc-temp-bucket") # TODO: Make configurable
            
            if project_id:
                writer = writer.option("parentProject", project_id)
            
            # Save mode
            writer.mode(mode).save(full_table_id)
            
        else:
            raise ValueError(f"Unsupported sink database type: {db_type}")

    @staticmethod
    async def execute_pipeline(pipeline_id: int, db_session):
        """
        Execute an ETL pipeline.
        """
        from app.models.etl import ETLPipeline, ETLDataSource
        from sqlalchemy import select
        import networkx as nx
        
        # 1. Fetch pipeline
        result = await db_session.execute(
            select(ETLPipeline).where(ETLPipeline.id == pipeline_id)
        )
        pipeline = result.scalar_one_or_none()
        if not pipeline:
            raise ValueError(f"Pipeline {pipeline_id} not found")
            
        graph = pipeline.graph_data
        if not graph or 'nodes' not in graph or 'edges' not in graph:
            raise ValueError("Invalid pipeline graph structure")
            
        # 2. Build DAG using NetworkX
        G = nx.DiGraph()
        for node in graph['nodes']:
            G.add_node(node['id'], **node)
        for edge in graph['edges']:
            G.add_edge(edge['source'], edge['target'])
            
        try:
            execution_order = list(nx.topological_sort(G))
        except nx.NetworkXUnfeasible:
            raise ValueError("Pipeline has cycles!")
            
        # 3. Execute Nodes
        # Store DataFrames in a dictionary keyed by Node ID
        node_results = {}
        
        # Initialize Spark
        spark = ETLService.get_spark_session()
        
        try:
            for node_id in execution_order:
                node = G.nodes[node_id]
                node_type = node.get('type')
                node_data = node.get('data', {})
                
                print(f"Executing node {node_id} ({node_type})...")
                
                if node_type == 'source':
                    datasource_id = node_data.get('datasourceId')
                    selected_columns = node_data.get('selectedColumns')
                    
                    # Fetch datasource
                    ds_result = await db_session.execute(
                        select(ETLDataSource).where(ETLDataSource.id == datasource_id)
                    )
                    datasource = ds_result.scalar_one_or_none()
                    if not datasource:
                        raise ValueError(f"Datasource {datasource_id} not found")
                        
                    df = ETLService.load_source_data(datasource, selected_columns)
                    node_results[node_id] = df
                    
                elif node_type == 'transform':
                    prompt = node_data.get('prompt')
                    generated_code = node_data.get('generatedCode')
                    
                    # Find upstream input DataFrame
                    upstream_nodes = list(G.predecessors(node_id))
                    if not upstream_nodes:
                        raise ValueError(f"Transform node {node_id} has no input")
                    
                    # Assume single input for now
                    input_df = node_results[upstream_nodes[0]]
                    
                    if not generated_code:
                         raise ValueError(f"Transform node {node_id} has no generated code")
                    
                    # Execute transformation
                    local_vars = {}
                    exec(generated_code, globals(), local_vars)
                    transform_func = local_vars.get('transform')
                    
                    if not transform_func:
                        raise ValueError(f"No 'transform' function found in generated code for node {node_id}")
                    
                    result_df = transform_func(spark, input_df)
                    node_results[node_id] = result_df
                    
                elif node_type == 'sink':
                    datasource_id = node_data.get('datasourceId')
                    table_name = node_data.get('tableName')
                    write_mode = node_data.get('writeMode', 'append')
                    
                    # Find upstream input
                    upstream_nodes = list(G.predecessors(node_id))
                    if not upstream_nodes:
                        raise ValueError(f"Sink node {node_id} has no input")
                        
                    input_df = node_results[upstream_nodes[0]]
                    
                    # Fetch datasource
                    ds_result = await db_session.execute(
                        select(ETLDataSource).where(ETLDataSource.id == datasource_id)
                    )
                    datasource = ds_result.scalar_one_or_none()
                    if not datasource:
                        raise ValueError(f"Datasource {datasource_id} not found")
                        
                    ETLService.write_sink_data(input_df, datasource, table_name, mode=write_mode)
                    
            return {"status": "success", "message": "Pipeline executed successfully"}
            
        except Exception as e:
            print(f"Pipeline execution failed: {e}")
            import traceback
            traceback.print_exc()
            raise Exception(f"Pipeline execution failed: {str(e)}")
